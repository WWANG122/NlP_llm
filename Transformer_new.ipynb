{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-08T15:57:38.780765Z",
     "start_time": "2024-10-08T15:57:38.757420Z"
    }
   },
   "source": [
    "import  torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 关于word embedding 以序列建模为例\n",
    "# 考虑source sentence 和 target sentence\n",
    "# 构建序列，序列的字符以索引的形式表示\n",
    "batch_size =2\n",
    "#单词表大小 \n",
    "max_num_src_words = 8\n",
    "max_num_trg_words = 8\n",
    "\n",
    "#特征大小\n",
    "model_dim = 8\n",
    "\n",
    "#序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_trg_seq_len = 5\n",
    "max_position_len = 5\n",
    "\n",
    "#src_len = torch.randint(2, 5, (batch_size,))\n",
    "#trg_len = torch.randint(2, 5, (batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "print(src_len)\n",
    "#单词索引构成的源句子和目标句子，并且做了padding，默认为0\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)),(0,max_src_seq_len-L)),0) for L in src_len] )\n",
    "\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_trg_words, (L,)),(0,max_trg_seq_len-L)),0) for L in tgt_len] )\n",
    "\n",
    "#构建embedding \n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim)\n",
    "trg_embedding_table = nn.Embedding(max_num_trg_words+1, model_dim)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = trg_embedding_table(tgt_seq)\n",
    "\n",
    "#构造position embedding \n",
    "pos_Mat = torch.arange(max_position_len).reshape((-1,1))\n",
    "i_mat =torch.pow(10000,torch.arange(0,8,2).reshape((1,-1))/model_dim) \n",
    "pe_embedding_table = torch.zeros(max_position_len,model_dim)\n",
    "pe_embedding_table[:,0::2] = torch.sin(pos_Mat/i_mat)\n",
    "pe_embedding_table[:,1::2] = torch.cos(pos_Mat/i_mat)\n",
    "print(pos_Mat)\n",
    "print(pe_embedding_table)\n",
    "\n",
    "\n",
    "# \n",
    "# print(src_embedding_table.weight)\n",
    "# print(src_seq)\n",
    "# print(src_embedding)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4], dtype=torch.int32)\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T14:13:09.346667Z",
     "start_time": "2024-10-08T14:13:09.310588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 设置参数\n",
    "max_position_len = 5  # 句子长度\n",
    "embedding_dim = 7     # 嵌入向量维度\n",
    "\n",
    "# 创建位置索引列向量\n",
    "pos_Mat = torch.arange(max_position_len).unsqueeze(1)  # [seq_len, 1]\n",
    "\n",
    "# 创建嵌入向量矩阵，这里我们随机生成一些值\n",
    "embeddings = torch.randn(max_position_len, embedding_dim)  # [seq_len, embedding_dim]\n",
    "\n",
    "# 创建位置编码矩阵\n",
    "imat = torch.pow(10000, -torch.arange(0, embedding_dim, 2).div(embedding_dim))\n",
    "peembeddingtable = torch.zeros(max_position_len, embedding_dim)\n",
    "peembeddingtable[:, 0::2] = torch.sin(pos_Mat * imat)\n",
    "peembeddingtable[:, 1::2] = torch.cos(pos_Mat * imat)\n",
    "\n",
    "# 将位置编码添加到嵌入向量上\n",
    "positional_embeddings = embeddings + peembeddingtable\n",
    "\n",
    "# 打印结果\n",
    "print(\"Position Indexes (as column vector):\\n\", pos_Mat)\n",
    "print(\"\\nEmbedding Vectors:\\n\", embeddings)\n",
    "print(\"\\nPositional Embedding Table:\\n\", peembeddingtable)\n",
    "print(\"\\nPositional Embeddings (Embeddings + Positional Embedding Table):\\n\", positional_embeddings)\n"
   ],
   "id": "26055b279cd51909",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [5, 3].  Tensor sizes: [5, 4]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m peembeddingtable \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(max_position_len, embedding_dim)\n\u001B[0;32m     16\u001B[0m peembeddingtable[:, \u001B[38;5;241m0\u001B[39m::\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msin(pos_Mat \u001B[38;5;241m*\u001B[39m imat)\n\u001B[1;32m---> 17\u001B[0m \u001B[43mpeembeddingtable\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcos(pos_Mat \u001B[38;5;241m*\u001B[39m imat)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# 将位置编码添加到嵌入向量上\u001B[39;00m\n\u001B[0;32m     20\u001B[0m positional_embeddings \u001B[38;5;241m=\u001B[39m embeddings \u001B[38;5;241m+\u001B[39m peembeddingtable\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [5, 3].  Tensor sizes: [5, 4]"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
